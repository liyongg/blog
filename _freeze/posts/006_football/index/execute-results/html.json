{
  "hash": "b2521253c33aa5da139ece1d76dc5d0b",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Deploying an MLflow Model with Streamlit: predicting footballer positions\"\ndate: \"2024-04-18\"\ncategories: [deployment, mlflow, classification, machine learning]\ndescription: \"GK, DF, MF, or FW?\"\ncode-fold: false\ncode-annotations: hover\n---\n\nIn the past I have done quite a few classification problems, but most of them were a code-along or done with trivial datasets that nearly every data scientist has done.\nWith the expanded responsibilities as a data scientist, which already are broadly defined across the industry, I wanted to write about deploying a model with an interface.\nIt is not uncommon to deploy models by serving them as a REST API, but I would only do that for models that could be used at larges scale.\nWe will be using *MLflow* to log our model, both *PySpark* (on Databricks) and the *scikit-learn* framework to train a model, *Streamlit* for the interface of the app, and deployment using *Docker* on my Raspberry Pi.\nBecause we use an interface and do not serve this as an API, some additional and or different steps need to be taken compared to only serving it using *FastAPI* or *Flask* on a remote server.\n\nToday we will train and deploy a model that can predict based on footballing stats whether a player is a goalkeeper, defender, midfielder or a forward.\nI have always had the idea to incorporate football into these types of projects and found this project to be the perfect use case.\nOur aim in this post is to create a model decent enough to deploy, but ot does not necessarily have to be groundbreaking across all metrics.\nLet's do it!\n\n## Step 1: Finding a dataset\n\n*Kaggle* is the perfect platform to find for these niche datasets.\nWhat is Kaggle?\n\n> Kaggle is the world's largest data science community with powerful tools and resources to help you achieve your data science goals.\n\nIt also has various user-contributed datasets.\nThe one we will be using today is [2023-2023 Football Player Stats](https://www.kaggle.com/datasets/vivovinco/20222023-football-player-stats).\n\n::: callout-note\nThere is also an API to download datasets from Kaggle, but it is a little overkill for a small project like this.\nIt would make more sense if this dataset was continuously updated, but it will not be as it only concerns 2022-2023 season data.\n:::\n\n## Step 2: Exploring context of the data\n\n> This dataset contains 2022-2023 football player stats per 90 minutes.\\\n>\n> Only players of Premier League, Ligue 1, Bundesliga, Serie A and La Liga are listed.\n\nThese competitions are usually denoted as the Top 5 European leagues, which is based on the [UEFA rankings](https://www.uefa.com/nationalassociations/uefarankings/country/?year=2023).\nComparing a player that has played 90 minutes and has attempted 50 passes to a player that has played 45 minutes and also attempted 50 passes would be unfair.\nLet's read the data\n\n::: panel-tabset\n## Pandas\n\n::: {#3c10a62a .cell execution_count=1}\n``` {.python .cell-code}\nimport pandas as pd\ndf = pd.read_csv(\"2022_2023_Football_Player_Stats.csv\", sep=\";\", encoding=\"latin-1\")\ndf.head()\n```\n\n::: {.cell-output .cell-output-display execution_count=28}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Rk</th>\n      <th>Player</th>\n      <th>Nation</th>\n      <th>Pos</th>\n      <th>Squad</th>\n      <th>Comp</th>\n      <th>Age</th>\n      <th>Born</th>\n      <th>MP</th>\n      <th>Starts</th>\n      <th>...</th>\n      <th>Off</th>\n      <th>Crs</th>\n      <th>TklW</th>\n      <th>PKwon</th>\n      <th>PKcon</th>\n      <th>OG</th>\n      <th>Recov</th>\n      <th>AerWon</th>\n      <th>AerLost</th>\n      <th>AerWon%</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>Brenden Aaronson</td>\n      <td>USA</td>\n      <td>MFFW</td>\n      <td>Leeds United</td>\n      <td>Premier League</td>\n      <td>22</td>\n      <td>2000</td>\n      <td>20</td>\n      <td>19</td>\n      <td>...</td>\n      <td>0.17</td>\n      <td>2.54</td>\n      <td>0.51</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.00</td>\n      <td>4.86</td>\n      <td>0.34</td>\n      <td>1.19</td>\n      <td>22.2</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>Yunis Abdelhamid</td>\n      <td>MAR</td>\n      <td>DF</td>\n      <td>Reims</td>\n      <td>Ligue 1</td>\n      <td>35</td>\n      <td>1987</td>\n      <td>22</td>\n      <td>22</td>\n      <td>...</td>\n      <td>0.05</td>\n      <td>0.18</td>\n      <td>1.59</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.00</td>\n      <td>6.64</td>\n      <td>2.18</td>\n      <td>1.23</td>\n      <td>64.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>Himad Abdelli</td>\n      <td>FRA</td>\n      <td>MFFW</td>\n      <td>Angers</td>\n      <td>Ligue 1</td>\n      <td>23</td>\n      <td>1999</td>\n      <td>14</td>\n      <td>8</td>\n      <td>...</td>\n      <td>0.00</td>\n      <td>1.05</td>\n      <td>1.40</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.00</td>\n      <td>8.14</td>\n      <td>0.93</td>\n      <td>1.05</td>\n      <td>47.1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>Salis Abdul Samed</td>\n      <td>GHA</td>\n      <td>MF</td>\n      <td>Lens</td>\n      <td>Ligue 1</td>\n      <td>22</td>\n      <td>2000</td>\n      <td>20</td>\n      <td>20</td>\n      <td>...</td>\n      <td>0.00</td>\n      <td>0.35</td>\n      <td>0.80</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.05</td>\n      <td>6.60</td>\n      <td>0.50</td>\n      <td>0.50</td>\n      <td>50.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>Laurent Abergel</td>\n      <td>FRA</td>\n      <td>MF</td>\n      <td>Lorient</td>\n      <td>Ligue 1</td>\n      <td>30</td>\n      <td>1993</td>\n      <td>15</td>\n      <td>15</td>\n      <td>...</td>\n      <td>0.00</td>\n      <td>0.23</td>\n      <td>2.02</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.00</td>\n      <td>6.51</td>\n      <td>0.31</td>\n      <td>0.39</td>\n      <td>44.4</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 124 columns</p>\n</div>\n```\n:::\n:::\n\n\n## PySpark\n\n::: {#6326786d .cell execution_count=2}\n``` {.python .cell-code}\nfrom pyspark.sql import SparkSession\nspark = SparkSession.builder.appName(\"football\").getOrCreate()\n\ndf = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").load(\"dbfs:/FileStore/tables/2022_2023_Football_Player_Stats.csv\", sep=\";\")\ndf.head()\n```\n:::\n\n\n:::\n\nThe dataset contains 124 columns, and we will not use all of them for the model.\n\n::: {.callout-note collapse=\"true\"}\n## Column descriptions\n\n-   Rk : Rank\n-   Player : Player's name\n-   Nation : Player's nation\n-   Pos : Position\n-   Squad : Squad’s name\n-   Comp : League that squat occupies\n-   Age : Player's age\n-   Born : Year of birth\n-   MP : Matches played\n-   Starts : Matches started\n-   Min : Minutes played\n-   90s : Minutes played divided by 90\n-   Goals : Goals scored or allowed\n-   Shots : Shots total (Does not include penalty kicks)\n-   SoT : Shots on target (Does not include penalty kicks)\n-   SoT% : Shots on target percentage (Does not include penalty kicks)\n-   G/Sh : Goals per shot\n-   G/SoT : Goals per shot on target (Does not include penalty kicks)\n-   ShoDist : Average distance, in yards, from goal of all shots taken (Does not include penalty kicks)\n-   ShoFK : Shots from free kicks\n-   ShoPK : Penalty kicks made\n-   PKatt : Penalty kicks attempted\n-   PasTotCmp : Passes completed\n-   PasTotAtt : Passes attempted\n-   PasTotCmp% : Pass completion percentage\n-   PasTotDist : Total distance, in yards, that completed passes have traveled in any direction\n-   PasTotPrgDist : Total distance, in yards, that completed passes have traveled towards the opponent's goal\n-   PasShoCmp : Passes completed (Passes between 5 and 15 yards)\n-   PasShoAtt : Passes attempted (Passes between 5 and 15 yards)\n-   PasShoCmp% : Pass completion percentage (Passes between 5 and 15 yards)\n-   PasMedCmp : Passes completed (Passes between 15 and 30 yards)\n-   PasMedAtt : Passes attempted (Passes between 15 and 30 yards)\n-   PasMedCmp% : Pass completion percentage (Passes between 15 and 30 yards)\n-   PasLonCmp : Passes completed (Passes longer than 30 yards)\n-   PasLonAtt : Passes attempted (Passes longer than 30 yards)\n-   PasLonCmp% : Pass completion percentage (Passes longer than 30 yards)\n-   Assists : Assists\n-   PasAss : Passes that directly lead to a shot (assisted shots)\n-   Pas3rd : Completed passes that enter the 1/3 of the pitch closest to the goal\n-   PPA : Completed passes into the 18-yard box\n-   CrsPA : Completed crosses into the 18-yard box\n-   PasProg : Completed passes that move the ball towards the opponent's goal at least 10 yards from its furthest point in the last six passes, or any completed pass into the penalty area\n-   PasAtt : Passes attempted\n-   PasLive : Live-ball passes\n-   PasDead : Dead-ball passes\n-   PasFK : Passes attempted from free kicks\n-   TB : Completed pass sent between back defenders into open space\n-   Sw : Passes that travel more than 40 yards of the width of the pitch\n-   PasCrs : Crosses\n-   TI : Throw-Ins taken\n-   CK : Corner kicks\n-   CkIn : Inswinging corner kicks\n-   CkOut : Outswinging corner kicks\n-   CkStr : Straight corner kicks\n-   PasCmp : Passes completed\n-   PasOff : Offsides\n-   PasBlocks : Blocked by the opponent who was standing it the path\n-   SCA : Shot-creating actions\n-   ScaPassLive : Completed live-ball passes that lead to a shot attempt\n-   ScaPassDead : Completed dead-ball passes that lead to a shot attempt\n-   ScaDrib : Successful dribbles that lead to a shot attempt\n-   ScaSh : Shots that lead to another shot attempt\n-   ScaFld : Fouls drawn that lead to a shot attempt\n-   ScaDef : Defensive actions that lead to a shot attempt\n-   GCA : Goal-creating actions\n-   GcaPassLive : Completed live-ball passes that lead to a goal\n-   GcaPassDead : Completed dead-ball passes that lead to a goal\n-   GcaDrib : Successful dribbles that lead to a goal\n-   GcaSh : Shots that lead to another goal-scoring shot\n-   GcaFld : Fouls drawn that lead to a goal\n-   GcaDef : Defensive actions that lead to a goal\n-   Tkl : Number of players tackled\n-   TklWon : Tackles in which the tackler's team won possession of the ball\n-   TklDef3rd : Tackles in defensive 1/3\n-   TklMid3rd : Tackles in middle 1/3\n-   TklAtt3rd : Tackles in attacking 1/3\n-   TklDri : Number of dribblers tackled\n-   TklDriAtt : Number of times dribbled past plus number of tackles\n-   TklDri% : Percentage of dribblers tackled\n-   TklDriPast : Number of times dribbled past by an opposing player\n-   Blocks : Number of times blocking the ball by standing in its path\n-   BlkSh : Number of times blocking a shot by standing in its path\n-   BlkPass : Number of times blocking a pass by standing in its path\n-   Int : Interceptions\n-   Tkl+Int : Number of players tackled plus number of interceptions\n-   Clr : Clearances\n-   Err : Mistakes leading to an opponent's shot\n-   Touches : Number of times a player touched the ball. Note: Receiving a pass, then dribbling, then sending a pass counts as one touch\n-   TouDefPen : Touches in defensive penalty area\n-   TouDef3rd : Touches in defensive 1/3\n-   TouMid3rd : Touches in middle 1/3\n-   TouAtt3rd : Touches in attacking 1/3\n-   TouAttPen : Touches in attacking penalty area\n-   TouLive : Live-ball touches. Does not include corner kicks, free kicks, throw-ins, kick-offs, goal kicks or penalty kicks.\n-   ToAtt : Number of attempts to take on defenders while dribbling\n-   ToSuc : Number of defenders taken on successfully, by dribbling past them\n-   ToSuc% : Percentage of take-ons Completed Successfully\n-   ToTkl : Number of times tackled by a defender during a take-on attempt\n-   ToTkl% : Percentage of time tackled by a defender during a take-on attempt\n-   Carries : Number of times the player controlled the ball with their feet\n-   CarTotDist : Total distance, in yards, a player moved the ball while controlling it with their feet, in any direction\n-   CarPrgDist : Total distance, in yards, a player moved the ball while controlling it with their feet towards the opponent's goal\n-   CarProg : Carries that move the ball towards the opponent's goal at least 5 yards, or any carry into the penalty area\n-   Car3rd : Carries that enter the 1/3 of the pitch closest to the goal\n-   CPA : Carries into the 18-yard box\n-   CarMis : Number of times a player failed when attempting to gain control of a ball\n-   CarDis : Number of times a player loses control of the ball after being tackled by an opposing player\n-   Rec : Number of times a player successfully received a pass\n-   RecProg : Completed passes that move the ball towards the opponent's goal at least 10 yards from its furthest point in the last six passes, or any completed pass into the penalty area\n-   CrdY : Yellow cards\n-   CrdR : Red cards\n-   2CrdY : Second yellow card\n-   Fls : Fouls committed\n-   Fld : Fouls drawn\n-   Off : Offsides\n-   Crs : Crosses\n-   TklW : Tackles in which the tackler's team won possession of the ball\n-   PKwon : Penalty kicks won\n-   PKcon : Penalty kicks conceded\n-   OG : Own goals\n-   Recov : Number of loose balls recovered\n-   AerWon : Aerials won\n-   AerLost : Aerials lost\n-   AerWon% : Percentage of aerials won\n:::\n\nThe variable that we want to predict is in the `Pos` column.\nHowever, a quick glance shows that some players have more than one position defined.\n\n::: panel-tabset\n## Pandas\n\n::: {#e840995e .cell execution_count=3}\n``` {.python .cell-code}\ndf[\"Pos\"].unique().tolist()\n```\n\n::: {.cell-output .cell-output-display execution_count=29}\n```\n['MFFW', 'DF', 'MF', 'FWMF', 'FW', 'DFFW', 'MFDF', 'GK', 'DFMF', 'FWDF']\n```\n:::\n:::\n\n\n## PySpark\n\n::: {#de2d8978 .cell execution_count=4}\n``` {.python .cell-code}\ndf.select(\"Pos\").distinct().show()\n```\n:::\n\n\n:::\n\nSome players play as midfielder (`MF`) but also occasionally as a defender (`DF`), denoted by `MFDF`.\nAnd what about players that have not played many matches?\nOn to some feature engineering!\n\n## Step 3: Feature engineering\n\nFirst thing we will do is filter the players that have played at least 5 matches.\n\n::: panel-tabset\n## Pandas\n\n::: {#f23ee251 .cell execution_count=5}\n``` {.python .cell-code}\ndf = df[df[\"MP\"] >= 5]\ndf.shape\n```\n\n::: {.cell-output .cell-output-display execution_count=30}\n```\n(2066, 124)\n```\n:::\n:::\n\n\n## PySpark\n\n::: {#04166534 .cell execution_count=6}\n``` {.python .cell-code}\ndf = df.filter(df[\"MP\"] >= 5)\ndf.count()\n```\n:::\n\n\n:::\n\nWe still have 2066 observations after filtering, which is fine.\nSecond thing to do is get rid of multiple positions for one player.\n\n::: panel-tabset\n## Pandas\n\n::: {#93289ef9 .cell execution_count=7}\n``` {.python .cell-code}\ndf[\"Pos\"].apply(len).unique().tolist()\n```\n\n::: {.cell-output .cell-output-display execution_count=31}\n```\n[4, 2]\n```\n:::\n:::\n\n\n## PySpark\n\n::: {#09362138 .cell execution_count=8}\n``` {.python .cell-code}\nfrom pyspark.sql.functions import length\n\ndf.withColumn(\"PosMod\", substring(col(\"Pos\"), 1, 2)).select(\"PosMod\").distinct().collect()\n```\n:::\n\n\n:::\n\nThe output tells us that `Pos` has either length 2 or 4.\nFor the sake of simplicity we will use the first two letters and define that as the position of the player.\n\n::: panel-tabset\n## Pandas\n\n::: {#32936617 .cell execution_count=9}\n``` {.python .cell-code}\ndf.loc[:, \"PosMod\"] = df[\"Pos\"].str[:2]\ndf[\"PosMod\"].unique().tolist()\n```\n\n::: {.cell-output .cell-output-display execution_count=32}\n```\n['MF', 'DF', 'FW', 'GK']\n```\n:::\n:::\n\n\n## PySpark\n\n::: {#8e05169b .cell execution_count=10}\n``` {.python .cell-code}\nfrom pyspark.sql.functions import col\n\ndf = df.withColumn(\"PosMod\", substring(col(\"Pos\"), 1, 2))\ndf.select(\"PosMod\").distinct().show()\n```\n:::\n\n\n:::\n\n4 positions, perfect!\nThe final thing to do with the features is encode the positions into numerical values.\n\n::: panel-tabset\n## Scikit-learn\n\n::: {#55203c6a .cell execution_count=11}\n``` {.python .cell-code}\nfrom sklearn.preprocessing import LabelEncoder\n\nlabel_encoder = LabelEncoder()\ndf[\"Class\"] = label_encoder.fit_transform(df[\"PosMod\"])\nlabel_encoder.classes_.tolist()\n```\n\n::: {.cell-output .cell-output-display execution_count=33}\n```\n['DF', 'FW', 'GK', 'MF']\n```\n:::\n:::\n\n\n## PySpark\n\n::: {#e04dbdc8 .cell execution_count=12}\n``` {.python .cell-code}\nfrom pyspark.ml.feature import StringIndexer\n\nindexer = StringIndexer(inputCol=\"PosMod\", outputCol=\"Class\")\ndf = indexer.fit(df).transform(df)\n```\n:::\n\n\n:::\n\nThe encoding is as follows\n\n| Position (Class) | Encoding |\n|------------------|----------|\n| DF = Defender    | 0        |\n| FW = Forward     | 1        |\n| GK = Goalkeeper  | 2        |\n| MF = Midfielder  | 3        |\n\nIt's extremely frustrating and it grinds my gears, but I don't think there's a way to insert a custom mapping.\nI would have preferred that the order would be GK - DF - MF - FW.\nMakes sense right?\n\nWhat remains is to define the columns to use and split the data for training and testing.\nWith some trial and error I found a selection that leads to a decent model.\nRemember, the interface should be user-friendly which means that I do not want to many input variables.\n\n::: panel-tabset\n## Scikit-learn\n\n::: {#126aa5ef .cell execution_count=13}\n``` {.python .cell-code}\ncols = [\"Shots\", \"PasMedAtt\", \"Pas3rd\", \"Clr\"]\nX = df[cols]\ny = df[\"Class\"]\n\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=10)\n```\n:::\n\n\n## PySpark\n\n::: {#79cf0587 .cell execution_count=14}\n``` {.python .cell-code}\ncols = [\"Shots\", \"PasMedAtt\", \"Pas3rd\", \"Clr\", \"Class\"]\ntrain, test = df.select(cols).randomSplit([0.7, 0.3], seed=10)\n```\n:::\n\n\n:::\n\n| Column     | Description                                                                         |\n|----------------------|--------------------------------------------------|\n| Shots      | Shots total (Does not include penalty kicks) per 90 minutes                         |\n| PassMedAtt | Passes attempted (Passes between 15 and 30 yards) per 90 minutes                    |\n| Pas3rd     | Completed passes that enter the 1/3 of the pitch closest to the goal per 90 minutes |\n| Clr        | Clearances per 90 minutes                                                           |\n\nIn another post I would like to revise this selection with grid optimisation.\nFor the current post the selection suffices.\n\n## Step 4: Training the model\n\nBefore training the model, we should scale the columns as players usually have many more passes attempted than for example shots or clearances.\n\n::: panel-tabset\n## Scikit-learn\n\n::: {#a22123fc .cell execution_count=15}\n``` {.python .cell-code}\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n```\n:::\n\n\n## PySpark\n\n::: {#8dd6a0d5 .cell execution_count=16}\n``` {.python .cell-code}\nfrom pyspark.ml.feature import StandardScaler, VectorAssembler\n\nassembler = VectorAssembler(inputCols=cols, outputCol=\"features_to_scale\")\ntrain = assembler.transform(train)\ntest = assembler.transform(test)\n\nscaler = StandardScaler(inputCol=\"features_to_scale\", outputCol=\"scaled_features\")\nscaler_model = scaler.fit(train)\ntrain_scaled = scaler_model.transform(train)\ntest_scaled = scaler_model.transform(test)\n```\n:::\n\n\n:::\n\nNow comes the part where we integrate MLflow.\nAgain, although MLflow allows us to evaluate iterations (runs) of models, for now we will use it to log a `.pkl` file which we can load into our Streamlit app.\nWe will be using a Random Forest classifier.\n\n::: callout-note\n### MLflow autologging in Databricks\n\nFor PySpark in Databricks, we do not need to explicitly call the MLflow functions.\nDatabricks has MLflow autologging enabled by default on ML clusters, which we are using.\n:::\n\n::: panel-tabset\n## Scikit-learn\n\n::: {#f8f33a8a .cell message='false' execution_count=17}\n``` {.python .cell-code}\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score, f1_score, recall_score\nimport mlflow.sklearn\n\nwith mlflow.start_run():\n    \n    mlflow.sklearn.log_model(scaler, \"rf_scaler\")\n    \n    # Train Random Forest classifier\n    rf = RandomForestClassifier(random_state=10)\n    rf.fit(X_train_scaled, y_train)\n\n    # Predict on test set\n    y_pred = rf.predict(X_test_scaled)\n\n    # Evaluate the model\n    f1 = f1_score(y_test, y_pred, average=\"weighted\")\n    accuracy = accuracy_score(y_test, y_pred)\n    recall = recall_score(y_test, y_pred, average=\"weighted\")\n\n    # Log metrics to MLflow\n    mlflow.log_metric(\"f1_score\", f1)\n    mlflow.log_metric(\"accuracy\", accuracy)\n    mlflow.log_metric(\"recall\", recall)\n\n    # Log final model to MLflow\n    mlflow.sklearn.log_model(rf, \"rf_model\")\n\nprint(f\"F1 Score: {f1}\")\nprint(f\"Accuracy: {accuracy}\")\nprint(f\"Recall: {recall}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nF1 Score: 0.8171709415334705\nAccuracy: 0.817741935483871\nRecall: 0.817741935483871\n```\n:::\n:::\n\n\n## PySpark\n\n::: {#4f152cd3 .cell execution_count=18}\n``` {.python .cell-code}\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator\nfrom pyspark.ml.classification import RandomForestClassifier\n\nrf = RandomForestClassifier(featuresCol=\"scaled_features\", labelCol=\"Class\")\nrf_model = rf.fit(train_scaled)\nrf_results = rf_model.transform(test_scaled)\n\n\nf1 = MulticlassClassificationEvaluator(labelCol=\"Class\")\naccuracy = MulticlassClassificationEvaluator(\n    labelCol=\"Class\", metricName=\"accuracy\")\nrecall = MulticlassClassificationEvaluator(\n    labelCol=\"Class\", metricName=\"recallByLabel\")\n\nf1.evaluate(rf_results)\naccuracy.evaluate(rf_results)\nrecall.evaluate(rf_results)\n```\n:::\n\n\n:::\n\nThese metrics seem fine![^1]\nNot something to write home about if you'd ask me, but good enough.\nIt could definitely be improved upon, but again: not the scope of this post.\nThe model and the corresponding scaler are saved under a local directory `/mlruns/0/<run_id>/artifacts/`. We extract the corresponding `.pkl` files and put them in the top-level, where the other `.py` files reside as well.\n\n[^1]: Results of scikit-learn and PySpark can differ, but I am talking about the scikit-learn metrics.\n\n::: callout-note\n## Continuing with scikit-learn results\n\nAs I am using the Databricks Community version, I found it very hard (or perhaps it is impossible) to extract the files to a local machine.\nTherefore we continue with scikit-learn results and code output.\n:::\n\n## Step 5: Creating a Streamlit app\n\nIn other projects I have fiddled around with Streamlit and I really enjoy its simplicity.\nIt allows for quick deployment and has a decent default interface.\nRather than building the `app.py` piece by piece, I will provide the file and explain parts of the code that are not self-explanatory.\n\n``` {.python filename=\"app.py (1/2)\"}\nimport pickle\nimport pandas as pd\nimport streamlit as st\n\n# Load MLflow model and scaler\nwith open(\"rf_model.pkl\", \"rb\") as f:\n    model = pickle.load(f)\n\nwith open(\"rf_scaler.pkl\", \"rb\") as f:\n    scaler = pickle.load(f)\n\n# Load data\ndf = pd.read_csv(\"2022_2023_Football_Player_Stats.csv\", sep=\";\", encoding=\"latin1\")\n\n# Positions\npositions = {0: \"Defender\", 1: \"Forward\", 2: \"Goalkeeper\", 3: \"Midfielder\"}\n\n\n# Helper function to preprocess input data\ndef preprocess_input(data):\n    # Scale data\n    data = scaler.transform(data)\n    \n    return data\n\n\n# Function to predict using the model\ndef predict(data):\n    # Preprocess input data\n    X = preprocess_input(data)\n    \n    # Make predictions\n    predictions = model.predict(X)\n    \n    return predictions\n\n\n# Function to compare input with existing players # <1>\ndef compare(data, n): # <1>\n    cols = [\"Shots\", \"PasMedAtt\", \"Pas3rd\", \"Clr\"] # <1>\n    mse = ((df[[\"Shots\", \"PasMedAtt\", \"Pas3rd\", \"Clr\"]] - data.iloc[0]) ** 2).mean(axis=1) # <1>\n    \n    idxs = mse.nsmallest(n).index # <1>\n    return df.loc[idxs][[\"Player\", \"Age\", \"Pos\", \"Squad\", \"Comp\"] + cols] # <1>\n\n\n# Function to format a line of player information\ndef format_player(row):\n    return f\"*{row['Player']}*, a {row['Age']} year old {row['Pos']} playing for {row['Squad']} in {row['Comp']}.\"\n\n\n# Function that provides text for players similar to input # <2>\ndef text_similar_players(data): # <2>\n    data = data.reset_index() # <2>\n\n    text = 'The players closest to your input are:\\n' # <2>\n    for index, row in data.iterrows(): # <2>\n        player_info = format_player(row) # <2>\n        text += f\"{index + 1}. {player_info}\\n\" # <2>\n    \n    return text # <2>\n\n# ...\n```\n\n1.  Compare user input with all players in the dataset. Use the MSE as the metric to minimise and find the *n* closest/most similar players with respect to input statistics.\n2.  Provide a list of *n* players that are most similar to user input.\n\nHonestly not very much to explain.\n\n``` {.python filename=\"app.py (1/2)\"}\n# Streamlit UI\ndef main():\n    st.set_page_config(page_title=\"Classifying Footballers\", layout=\"wide\")\n    st.markdown(\"\"\" # <1>\n        <style> # <1>\n            .reportview-container { # <1>\n                margin-top: -2em; # <1>\n            } # <1>\n            #MainMenu {visibility: hidden;} # <1>\n            .stDeployButton {display:none;} # <1>\n            footer {visibility: hidden;} # <1>\n            #stDecoration {display:none;} # <1>\n        </style> # <1>\n    \"\"\", unsafe_allow_html=True) # <1>\n    \n    st.title(\"Predicting Football player positions based on 2022/23 stats\")\n    \n    st.sidebar.success(f\"Visit [blog.panliyong.nl](https://blog.panliyong.nl/posts/006_football) for the post!\")\n    \n    # Create input form # <2>\n    st.sidebar.header(\"Input Features\") # <2>\n    shots = st.sidebar.number_input(\"Shots\", min_value=0.0, max_value=10.0, step=0.1, value=1.0) # <2>\n    pas_med_att = st.sidebar.number_input(\"Passes Medium Att\", min_value=0.0, max_value=70.0, step=1.0, value=20.0) # <2>\n    pas_3rd = st.sidebar.number_input(\"3rd Passes\", min_value=0.0, max_value=20.0, step=1.0, value=3.0) # <2>\n    clr = st.sidebar.number_input(\"Clearances\", min_value=0.0, max_value=15.0, step=0.1, value=3.0) # <2>\n    \n    # Create column layout\n    left_column, right_column = st.columns(2)\n    \n    with left_column:\n        # Players compare\n        st.markdown(\"### Find similar players\")\n        n_players = st.slider(\"How many similar players do you want to find?\", min_value=1, max_value=20, step=1,\n                              value=5)\n    \n    # Create prediction button\n    if st.sidebar.button(\"Predict\"):\n        # Create a dataframe from the input\n        input_data = pd.DataFrame({\n            \"Shots\": [shots],\n            \"PasMedAtt\": [pas_med_att],\n            \"Pas3rd\": [pas_3rd],\n            \"Clr\": [clr]\n        })\n        \n        # Predict\n        prediction = predict(input_data)\n        \n        # Display prediction\n        position = positions.get(prediction[0])\n        position_styled = f\"<span style='color:purple'>{position}</span>\"\n        \n        st.sidebar.info(f\"Predicted Position: {position}\")\n        \n        # Check which player is closest to input\n        players = compare(input_data, n=n_players)\n        \n        text = text_similar_players(players)\n        \n        with left_column:\n            st.info(text)\n        \n        with right_column:\n            st.markdown(f\"# **Predicted a** {position_styled}\",\n                        unsafe_allow_html=True)\n            st.markdown(\"## Data of similar players:\")\n            st.dataframe(players.reset_index(drop=True))\n\n\n# Run the app\nif __name__ == \"__main__\":\n    main()\n```\n\n1.  Remove developer settings for Streamlit.\n2.  Define user input interactivity.\n\nWe can try and see if the Streamlit app runs with the following command\n\n``` {.bash filename=\"Terminal\"}\nstreamlit run app.py\n```\n\nThis is what the app looks like:\n\n![](app.png)\n\nNow it's time to prepare it for deployment!\n\n## Step 6: Make a Dockerfile\n\nA few things to keep in mind when writing this Dockerfile is that the `requirements.txt` and the other environment files stored by MLflow have many irrelevant dependencies.\nThe only things we need are `pandas`, `streamlit` and `scikit-learn`.\nThat makes building the Dockerfile quicker as there are less dependencies that have to be installed.\n\nHowever, another important factor is that all the time I have been developing on a Windows (64-bit) machine.\nThe Raspberry Pi which the app is going to run on is a Linux (arm64) machine.\nAs these platforms are not the same, we cannot simply use `docker build` commands and expect them to work on the Linux device.\n\nLet's first consider our Dockerfile:\n\n``` dockerfile\nFROM python:3.10-slim\n\nWORKDIR /app\n\nCOPY app.py .\nCOPY 2022_2023_Football_Player_Stats.csv .\nCOPY rf_model.pkl .\nCOPY rf_scaler.pkl .\n\nRUN pip install streamlit pandas scikit-learn==1.4.0\n\nEXPOSE 8501\nCMD [\"streamlit\", \"run\", \"app.py\", \"--server.address=0.0.0.0\"]\n```\n\nThe default port of Streamlit apps is 8501 and we did not change that, so we have to expose this port.\nFurthermore only the `.pkl` files and `app.py` and `.csv` file are all the files we need.\nThen we can install the libraries we need to run the Streamlit app.\n\nNow comes the part where we build the image and push it to Docker Hub under `panliyong/football-positions`.\nUp for grabs on the Raspberry Pi!\n\n``` {.bash filename=\"Terminal\"}\ndocker buildx build --platform linux/arm64 -t panliyong/football-positions -f Dockerfile --push .\n```\n\n## Step 7: Deploying on Raspberry Pi\n\nThe image is now on Docker Hub and we can pull the image now.\nLet's name the container `football-positions` and map port 8501 of the Pi to the exposed port.\nI have also had issues with containers running out of memory, so in order to keep this container running we can update the restart policy to always restart the container.[^2]\n\n[^2]: This could also be done in the initial run command, but I forgot and was too lazy to delete it...\n\n``` {.bash filename=\"Terminal\"}\ndocker pull panliyong/football-positions\ndocker run -d --name football-positions -p 8501:8501 panliyong/football-positions\ndocker update --restart=always football-positions\n```\n\nMy domain `panliyong.nl` and subdomains are managed through [Cloudflare](https://www.cloudflare.com/) and are hosted on the Apache web server on the same Pi.\nWe can serve this on [football-positions.panliyong.nl](https://football-positions.panliyong.nl) for the app.\nIf we go to the DNS settings and add a `CNAME` record with name `football-positions` and target `panliyong.nl`, we enable this new subdomain.\n\nWe need to add a virtual host `football-positions.conf` and enable it.\n\n``` {.bash filename=\"football-positions.conf\"}\n<VirtualHost *:80>\n    ServerName football-positions.panliyong.nl\n    ServerAlias www.football-positions.panliyong.nl\n\n    ProxyPass / http://localhost:8501/\n    ProxyPassReverse / http://localhost:8501/\n\n    # ...\n</VirtualHost>\n\n<IfModule mod_ssl.c>\n    <VirtualHost *:443>\n        ServerAlias www.football-positions.panliyong.nl\n        ServerName football-positions.panliyong.nl\n\n        SSLProxyEngine on\n        ProxyPass / http://localhost:8501/\n        ProxyPassReverse / http://localhost:8501/\n\n        # ...\n\n        # Redirect HTTP to HTTPS\n        RewriteEngine On\n        RewriteCond %{HTTPS} off\n        RewriteRule ^ https://%{HTTP_HOST}%{REQUEST_URI} [L,R=301]\n    </VirtualHost>\n</IfModule>\n```\n\nEnable it with these commands:\n\n``` bash\nsudo a2ensite football-positions.conf\nsudo systemctl reload apache2\n```\n\nIf everything is correctly set up the app should be accessible through [football-positions.panliyong.nl](https://football-positions.panliyong.nl).\n\n::: callout-caution\n## Inaccessible due to Streamlit updates\n\nIt appears that the app is not loading on the Apache web server due to websocket errors and some API changes.\nI'll be looking for a fix...\nFor the time being we can use [football-positions.streamlit.app](https://football-positions.streamlit.app/)\n:::\n\n## TL;DR\n\nIt's not as difficult as it sounds to prepare a model for deployment.\nDue to unforeseen changes in the `streamlit==1.33` package, the app did not load due to websocket erros.\nThe app is currently hosted on [football-positions.streamlit.app](https://football-positions.streamlit.app/).\nWe will be looking back on this soon!\n\nThat's all for today, thanks for reading!\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}