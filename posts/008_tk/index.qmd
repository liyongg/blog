---
title: "Visually finding a statistical *Toni Kroos* replacement"
date: "2024-09-24"
categories: [visualisation]
description: "Irreplacable"
code-fold: show
---

If you have watched any of Real Madrid's first few games in this season (2024/2025) and also previous seasons, you must have noticed a hasty and uncontrolled playing style in recent games.
Very clearly this is due to the absence of Toni Kroos in midfield, and you don't have to be a [*tactico*](https://www.urbandictionary.com/define.php?term=tactico) to observe that.
Even though he is truly one of a kind, I would like to use football data to find the most similar player in terms of statistics.
A recent trip to [PyData Amsterdam 2024](https://amsterdam.pydata.org/) and the talk of Jeroen Janssens about ['How I hacked UMAP and won at a plotting contest'](https://amsterdam2024.pydata.org/cfp/talk/SMB7J3/) has inspired me to try and play around with [Polars](https://pola.rs/) for a small side project, and this seems like the perfect one!
Also the use of [UMAP](https://umap-learn.readthedocs.io/en/latest/) suits this project particularly well.

![Finding a Toni Kroos replacement](https://media1.tenor.com/m/YQi-S1vE6eEAAAAC/i-need-that-we-need-that-man-city.gif)

In an earlier [post with football data](../006_football/index.qmd) we used a type of football data which is great for dimensionality reduction, which is what UMAP entails as a technique.
The only problem with that data is that it was for the 2022-2023 season, and I want to use the most recent completed season.
Fortunately the Kaggle dataset also showed the source and it was quite easy to find the source data for the 2023-2024 season.

## Step 1: Obtaining the data

After clicking around on [FBREF](https://fbref.com/en/comps/Big5/stats/players/Big-5-European-Leagues-Stats), I found the player stats of the 2023-2024 season, so let's get digging!
<!-- [](https://fbref.com/en/comps/Big5/2023-2024/stats/players/2023-2024-Big-5-European-Leagues-Stats) -->

![.. *Diglett Dig Diglett Dig Diglett Dig Diglett Dig* ...](https://media1.tenor.com/m/-k2lIRMzytEAAAAC/jukemon.gif)

The data is not yet in an easy format to work with.
We could opt for `pd.read_html()` method, but that's no fun!
Besides, I think doing manual HTML inspecting and then extracting is a good skill to maintain (from time to time).
As it is simply a matter of extracting some HTML content, `beautifulsoup` should be sufficient!

``` {.bash filename="Terminal"}
pip install bs4 polars
```

Let's first inspect the stats.
There are several categories with each their own URL:

-   standard stats
-   goalkeeping
-   advanced goalkeeping
-   shooting
-   passing
-   pass types
-   goal and shot creation
-   possession
-   playing time
-   miscellaneous stats

Even though Toni Kroos is not a goalkeeper, I think it's good to keep the goalkeeping stats for the final UMAP plot.
We will, however, disregard the 'standard stats' and 'miscellaneous stats'.
Fortunately the URLs are similar and only differ by a simple substring.

```{python}
import requests
from bs4 import BeautifulSoup
import polars as pl


def extract_table_and_columns(stat: str) -> (pl.DataFrame, dict):
    """Extract table from url.

    Args:
        stat (str): The statistic of the table

    Returns:
        pl.DataFrame: A Polars dataframe with the information of the players.
        dict: A dictionary of columns and their descriptions.
    """
    base_url = "https://fbref.com/en/comps/Big5/2023-2024/{stat}/players/2023-2024-Big-5-European-Leagues-Stats"
    url = base_url.format(stat=stat)
    resp = requests.get(url)
    soup = BeautifulSoup(resp.content, features="lxml")

    # The HTML of the table
    stats_table = soup.find_all("table")[-1]

    # Parsing headers
    tr_header = stats_table.findChildren(name="tr")[1]
    column_descriptions = {
        cell.get("aria-label"): cell.get("data-tip")
        for cell in tr_header.find_all(["th", "td"])
    }
    columns = [key for key in column_descriptions.keys()]

    if len(columns) != len(set(columns)):
        raise AssertionError("Column names must be unique.")

    # Parsing stats
    table_body = stats_table.find_next(name="tbody")
    rows_players = table_body.findChildren(name="tr", attrs={"class": False})

    table_data: list = []
    for row in rows_players:
        cells = row.find_all(
            ["th", "td"]
        )  # Collect all cell data, both headers and regular cells
        table_data.append([cell.get_text(strip=True) for cell in cells])

    return pl.DataFrame(table_data, schema=columns, orient="row"), column_descriptions
```

Let's look at the passing stats!

::: column-screen-inset
```{python}
df, descriptions = extract_table_and_columns("passing")
df
```
:::

Damn, looks kinda ugly with the automatically inferred strings for every column.
We are actually going to try something quick and nasty for this, like saving these as `.csv` files and then reading them again for a better inference.[^1]

[^1]: The only reason I put this up is because I spent too much time messing around with Polars, and I just did not get it to work...

::: column-screen-inset
```{python}
import os


def parse_columns(df: pl.DataFrame) -> pl.DataFrame:
    """Parse columns of a dataframe for better inference.

    Args:
        df (pl.DataFrame): A Polars dataframe which has structurally incorrect columns.

    Returns:
        pl.DataFrame: A Polars dataframe with better inferred column types.
    """
    df.write_csv("temp.csv", quote_style="necessary")
    parsed_df = pl.read_csv("temp.csv", use_pyarrow=True)
    if os.path.exists("temp.csv"):
        os.remove("temp.csv")
    return parsed_df


def extract_and_parse_table(stat) -> pl.DataFrame:
    df, descriptions = extract_table_and_columns(stat)
    return parse_columns(df)

extract_and_parse_table("passing")
```
:::

This is much better!
Now let's refactor a little and put this together into a class:

```{python}
class StatsExtractor:
    def __init__(self, stat: str):
        self.stat: str = stat
        self.base_url: str = (
            "https://fbref.com/en/comps/Big5/2023-2024/{stat}/players/2023-2024-Big-5-European-Leagues-Stats"
        )
        self.url: str = self.base_url.format(stat=self.stat)
        self.df = None
        self.column_descriptions = None
        self.fetch_data()
        self.extract_column_descriptions()
        self.extract_stats()
        self.parse_columns()

    def fetch_data(self) -> None:
        """Fetch the HTML content from the URL."""
        url = self.base_url.format(stat=self.stat)
        resp = requests.get(url)
        soup = BeautifulSoup(resp.content, features="lxml")
        self.html = soup.find_all("table")[-1]

    def extract_column_descriptions(self) -> None:
        """Extract column descriptions"""
        tr_header = self.html.findChildren(name="tr")[1]
        column_descriptions = {
            cell.get("aria-label"): cell.get("data-tip")
            for cell in tr_header.find_all(["th", "td"])
        }
        columns = [key for key in column_descriptions.keys()]

        if len(columns) != len(set(columns)):
            raise AssertionError("Column names must be unique.")
        self.columns = columns
        self.column_descriptions = column_descriptions

    def extract_stats(self) -> None:
        """Extract table from URL and store it in a DataFrame."""
        table_body = self.html.find_next(name="tbody")
        rows_players = table_body.findChildren(name="tr", attrs={"class": False})

        table_data: list = []
        for row in rows_players:
            cells = row.find_all(["th", "td"])
            table_data.append([cell.get_text(strip=True) for cell in cells])

        self.df = pl.DataFrame(table_data, schema=self.columns, orient="row")

    def parse_columns(self) -> None:
        """Parse columns of a DataFrame for better inference."""
        self.df.write_csv("temp.csv", quote_style="necessary")
        parsed_df = pl.read_csv("temp.csv", use_pyarrow=True)
        # if os.path.exists("temp.csv"):
            # os.remove("temp.csv")
        self.df = parsed_df
```

We can now invoke the methods after creating a `StatsExtractor` instance.

```{python}
stats_to_extract = [
        "passing",
        "keepers",
        "shooting",
        "passing_types",
        "gca",
        "defense",
        "possession",
]
extractors = {stat: StatsExtractor(stat) for stat in stats_to_extract}
dfs = {stat: extractor.df for stat, extractor in extractors.items()}
```

With `dfs` we only need to create one combined dataframe, and actually get to learn more about the syntax of `polars`.

## Step 2: Exploring `polars` by data wrangling

After viewing the documentation and some sample code, `polars` looks very similar to `dplyr` in *R* - and I have to say I am quite pleased.
My career started with loads of R and in my own time and projects I have transitioned more towards Python.
However, the one and only thing I really miss about R is how straightforward and concise the syntax is compared to `pandas`.
Take the following two dataframes

```{python}
df_passing = dfs["passing"]
df_possession = dfs["possession"]
```

I am so used to not having duplicate columns when performing joins.
However in Python using either `pandas` or `polars`, I have not found any other way than to drop the duplicate columns.
Let's join these dataframes on the columns `Player` and `Squad` [^2], and drop all the other duplicate columns.
In `polars` we can do so using the selector `cs.ends_with()`.

[^2]: Joining on the player name is trivial, but the squad is less obvious: the data could contain multiple records of the same player.
    This is due to either late transfers or a transfer in the winter window.

::: column-screen-inset
```{python}
import polars.selectors as cs

df_passing.join(df_possession, how="left", on=["Player", "Squad"]).drop(cs.ends_with("_right"))
```
:::

Exactly what I wanted!
Now we can iterate this process until there are no dataframes left to merge on.

::: column-screen-inset
```{python}
from functools import reduce

df_stats = reduce(
    lambda left, right: left.join(right, how="left", on=["Player", "Squad"]).drop(
        cs.ends_with("_right")
    ),
    dfs.values()
)
df_stats
```
:::

Now it's time to filter and select relevant features and change them slightly.
Before doing alterations, I think it is fair to only include players who have played more than 90 minutes 15 times.
Perhaps *fair* is not the right word, but including players who only played a handful of matches is misleading when evaluating their overall season performance.
It does not capture consistency which is what we *do* want to include.

::: column-screen-inset
```{python}
df_stats.filter(pl.col("90s Played") >= 15)
```
:::

Although players can play multiple positions, for the plots I prefer we keep it on a down low.
The `Position` column now has these unique values:

<div>

```{python}
df_stats.select(pl.col("Position")).unique().to_series().to_list()
```

</div>

For the players that play multiple positions, we take the position specified first assuming it is their primary position.
This will bring down the number of different positions to four!

<div>

```{python}
df_stats_relevant = (df_stats
    .filter(pl.col("90s Played") >= 15)
    .with_columns(pl.col("Position").str.slice(0, 2))
)
```

</div>

UMAP requires numerical data, so let's only select numerical columns, which in this case are all non-string columns.

::: column-screen-inset
```{python}
df_stats_relevant.select(~cs.by_dtype(pl.String))
```
:::

There are still some irrelevant columns, namely `Rank`, `Age` and `Year of Birth`.

::: column-screen-inset
```{python}
df_stats_relevant.select(
    ~cs.by_dtype(pl.String) & ~cs.by_name(["Age", "Rk", "Year of birth"])
)
```
:::

For a good comparison, it is fair to condition all stats per 90 minutes played.
I am happy to say there is a column for that, so we can divide each column by that column.

::: column-screen-inset
```{python}
df_stats_relevant.select(
    ~cs.by_dtype(pl.String) & ~cs.by_name(["Age", "Rk", "Year of birth"])
).with_columns(pl.all() / pl.col("90s Played").exclude("90s Played"))
```
:::

The stat of number of 90s played can be ignored if we disregard 'fitness' as a trait Toni Kroos had.
This brings us to the following dataframe:

::: column-screen-inset
```{python}
df_stats_relevant_umap = (
    df_stats_relevant.select(
        ~cs.by_dtype(pl.String) & ~cs.by_name(["Age", "Rk", "Year of birth"])
    )
    .with_columns(pl.all() / pl.col("90s Played").exclude("90s Played"))
    .drop(pl.col("90s Played"))
)
df_stats_relevant_umap
```
:::

There is only one thing left I would like to change.
For the plots in the next step, it is useful to highlight Toni Kroos.
By changing the value of the `Position` column, we can still use that column to dictate the colours and also have this clear distinction for Kroos.

```{python}
df_stats_relevant = df_stats_relevant.with_columns(
    pl.when(pl.col("Player") == "Toni Kroos")
    .then(pl.lit("Toni Kroos"))
    .otherwise(pl.col("Position"))
    .alias("Position")
)
```

## Step 3: Interpreting UMAP results and plots

We've got the dataframe and it is very tempting to just plug that into UMAP.
Yet it generally is wiser to scale the data first.
We can use the standard preprocessing tools that we know from `sklearn`.

```{python}
from sklearn.preprocessing import StandardScaler

df_stats_numeric_scaled = StandardScaler().fit_transform(df_stats_relevant_umap)
```

Let's get on with the first plot!

```{python}
#| warning: false
#| message: false
import umap.plot

reducer = umap.umap_.UMAP(random_state=10)
mapper = reducer.fit(df_stats_numeric_scaled, force_all_finite="allow-nan")
umap.plot.points(mapper, labels=df_stats_relevant["Position"], background="black")
```

Oh man...

![What's that?!](https://media1.tenor.com/m/n9wEAgmUjiEAAAAC/ew-spongebob.gif)

What even is this?!
Four positions...
we would expect four 'clusters', right?
Of course you can argue about the generality of the positions, but we would at least expect some type of separation between the positions.
The current one is all over the place.
But hope's not lost yet!
Checking the documentation of the arguments we can provide to UMAP shows 'euclidean' as default metric.
I would rather not go into too much detail, so essentially: the Euclidean metric in this case does not make sense.
It would mean that people that have similar 'mean' stats across all stats will be packed closer to each other.
This is not what we want: there should be more weights on the qualities that make a player unique, and that should be compared to others.
A better metric to use would be the *Chebyshev* metric.
Again without going into too much detail: the Chebyshev metric measures the greatest (absolute) difference between any stat of two players.
For example, if player A is a better dribbler and player B is a better passer, these individual qualities will be more prominent with the Chebyshev metric.
Let's see with a plot!

```{python}
#| warning: false
#| classes: preview-image
import umap.plot

reducer = umap.umap_.UMAP(random_state=10, metric="chebyshev")
mapper = reducer.fit(df_stats_numeric_scaled, force_all_finite="allow-nan")
umap.plot.points(mapper, labels=df_stats_relevant["Position"], background="black")
```

Ah, much better.

![Phew](https://media1.tenor.com/m/o1fnLBZm-OAAAAAC/phew-sigh.gif)

But still something we can improve on!
Now what about the other parameters?
We have already touched upon the `metric`.
The other frequently used parameters are:

-   `n_components`;
-   `n_neighbors`;
-   `min_dist`.

For the components we can show both a 2D and 3D case.
Plotting in higher dimensions is a little more cumbersome.
That leaves `n_neighbors` and `min_dist`.
The former is set at 15 by default.
A rule of thumb is that the lower this value, the more the local structure will be preserved.
Conversely the higher this number, the more UMAP sees the overall structure of the data.
After some tweaking[^3], it was clear from the results that a value of around 30 is quite good for both the local and global structure.

[^3]: You can do so by running a simple for loop and inspecting all the plotted results.

Afs for the `min_dist`, the rule of thumb is: the lower the value, the clumpier the embeddings.
As is stated by the documentation of UMAP[^4], this can be useful for clustering.
A default value of 0.1 is fine for our use case, which leads us to the following instances for 2D and 3D UMAP embeddings.

[^4]: <https://umap-learn.readthedocs.io/en/latest/parameters.html#n-neighbors>

```{python}
#| error: false
#| warning: false
reducer_2d = umap.umap_.UMAP(
    n_components=2, metric="chebyshev", n_neighbors=30, random_state=10
)
reducer_3d = umap.umap_.UMAP(
    n_components=3, metric="chebyshev", n_neighbors=30, random_state=10
)

embeddings_2d = reducer_2d.fit_transform(
    df_stats_numeric_scaled, force_all_finite="allow-nan"
)
embeddings_3d = reducer_3d.fit_transform(
    df_stats_numeric_scaled, force_all_finite="allow-nan"
)
```

This leads us to two interactive figures underneath!

```{python}
#| column: screen-inset-shaded
#| layout-nrow: 1
#| code-fold: true
import plotly.express as px

df_plot_2d = df_stats_relevant.hstack(pl.DataFrame(embeddings_2d, schema=["x", "y"]))
df_plot_3d = df_stats_relevant.hstack(
    pl.DataFrame(embeddings_3d, schema=["x", "y", "z"])
)

fig_2d = px.scatter(
    df_plot_2d,
    title="2D",
    x="x",
    y="y",
    color=df_stats_relevant["Position"],
    hover_data=["Player", "Squad", "Competition"],
    labels={"color": ""},
)
fig_2d.update_layout(
    plot_bgcolor="white",
    margin=dict(l=20, r=20, t=50, b=20),
    xaxis=dict(showticklabels=False, title=None, linecolor="black"),
    yaxis=dict(showticklabels=False, title=None, linecolor="black"),
)
fig_2d.update_traces(
    hovertemplate="<br>".join(
        [
            "Player: %{customdata[0]}",
            "Squad: %{customdata[1]}",
            "Competition: %{customdata[2]}",
        ]
    )
)

fig_3d = px.scatter_3d(
    df_plot_3d,
    title="3D",
    x="x",
    y="y",
    z="z",
    color=df_stats_relevant["Position"],
    hover_data=["Player", "Squad", "Competition"],
    labels={"color": ""},
)
fig_3d.update_traces(
    marker_size=5,
    hovertemplate="<br>".join(
        [
            "Player: %{customdata[0]}",
            "Squad: %{customdata[1]}",
            "Competition: %{customdata[2]}",
        ]
    ),
)
fig_3d.update_layout(
    scene=dict(
        xaxis_title="",
        yaxis_title="",
        zaxis_title="",
        xaxis_showspikes=False,
        yaxis_showspikes=False,
        xaxis=dict(
            backgroundcolor="rgb(200, 200, 230)",
            showticklabels=False,
            linecolor="black",
        ),
        yaxis=dict(
            backgroundcolor="rgb(230, 200,230)",
            showticklabels=False,
            linecolor="black",
        ),
        zaxis=dict(
            backgroundcolor="rgb(230, 230,200)",
            showticklabels=False,
            linecolor="black",
        ),
    ),
    margin=dict(l=20, r=20, t=50, b=20),
)

fig_2d.show()
fig_3d.show()
```

*Aleix Garcia* is statistically[^5] the most similar player to Toni Kroos, which can be seen in both the 2D and 3D embedding.
He is a central midfielder who played for Girona in the 2023/24 season.
If you followed *LaLiga* last leason you should know that Girona was a serious title contender for the majority of the run.
Only at the final third part did they not keep up with Real Madrid anymore.
There have been many praises for this Girona team, with Aleix Garcia as one of the most prominent players.
It does not surprise me the least that he is most similar to Kroos, and for his tremendous season he has earned a transfer to Bayer Leverkusen, the 2023/24 *Bundesliga* champions.

[^5]: According to the Chebyshev metric and our choice for `min_dist` and `n_neighbors`.

In the 3D embedding, it is quite extraordinary to see a defenders close to him as well.
The one that is closes is *Trent Alexander-Arnold*, who plays for Liverpool in the Premier League.
He is known to be an amazing creator, but usually plays right-back.
There are some rumours about him joining Real Madrid as a free agent in 2025, so who knows...
👀qu

## TL;DR

Aleix Garcia.
Trent Alexander-Arnold was the defender that was the closest.
Keep in mind that the source does not have *all* statistics.
Also we took the entirety of stats.
For a more scoped look in the key things that are missing for Real Madrid, we could opt to include only those stats, such as passing and possession.

That's all for today, thanks for reading!
