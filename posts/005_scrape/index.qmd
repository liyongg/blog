---
title: "Translating PDF content using an LLM"
date: "2024-04-03"
categories: [text mining, deep learning, pytorch]
description: "CN: Translating PDF content using an LLM!"
code-fold: false
---

I have an extremely interesting and ambitious project that I am working on where I use React Native for the front end, and FastAPI as the backend.
One of the things I am considering to do is to include the use of a Large Language Model (LLM).
LLMs have been the hype for quite some time now, and I thought it was time to put one to use, aside from the usual ChatGPT prompts that I run.

Today I will use an LLM to translate content that we will extract from a PDF.
Not very coincidentally it is a Dutch vocabulary list, which I want to translate to Mandarin Chinese.

## Step 1: Finding a dataset

The very first relevant hit on Google brought me to a [NT2 Vocabulary List](https://www.nt2.nl/documenten/dm-derde_ronde-woordenlijst-lowres.pdf){target="_blank"}.
Let's save this PDF as `vocab.pdf`.

## Step 2: Exploring the PDF using `pypdf`

```{python}
from pypdf import PdfReader
reader = PdfReader("vocab.pdf")
```

We should at least check if it indeed has read correctly that `vocab.pdf` contains 36 pages

```{python}
print(f"There are {len(reader.pages)} pages.")
```

Alright, seems good!

## Step 3: Extracting text

The `pages` value is a list of `PageObject` objects and each of these come with the `extract_text()` method.
The extracted content from the first page is the following:

```{python}
first_page = reader.pages[0]
text_first_page = first_page.extract_text()
print(text_first_page)
```

It is evident from scrolling through the PDF that it is quite well-structured: every word and its meaning start with some index.
This is reaffirmed with the string printed above.
However, if we would use the string representation above, it would be incredibly tedious to find some algorithm that can help extract the most important information: the words and their corresponding meaning.
One naive way would be to define

1.  the first word as the index;
2.  the second word as the word in the vocabulary list;
3.  the remaining words to be the corresponding meaning.

But this sucks.
To see that, let's first split the lines for this very long string and show the first 10 results.

```{python}
lines = text_first_page.splitlines()[:9]
print("\n".join(lines))
```

By using the naive way to divide the strings, the final line would give

| Index | Word  | Meaning                                                                                  |
|-----------------|-----------------|---------------------------------------|
| 5     | stelt | ... voor dat stelt niks voor = dat is helemaal niet belangrijk; voorstelde-stelde voor-' |

But the actual result should be

| Index | Word           | Meaning                                                                         |
|-----------------|-----------------|--------------------------------------|
| 5     | stelt ... voor | dat stelt niks voor = dat is helemaal niet belangrijk; voorstelde-stelde voor-' |

Not great, so we have to find something else.
The same method has an argument `extraction_mode` which is set to `plain` by default.
If we use the `extract_text(extraction_mode="layout")`, it allows us to apply a more rigid and robust method.

```{python}
text_first_page = first_page.extract_text(extraction_mode="layout")
lines = text_first_page.splitlines()[:9]
print("\n".join(lines))
```

Now there are a lot more whitespace characters between each 'column'.
A better - not necessarily the best - method would be to:

1.  Filter the lines which are non-empty and start with a digit.
2.  Aggregate the rows which belong to one word/meaning combination.
3.  Split each line by at least three or more whitespace characters.
4.  Define the first part as the word and define the remaining text as its meaning.
5.  Create a Pandas DataFrame object for each page.
6.  Concatenate all dataframes into one dataframe.

Let's set up this pipeline.

## Step 4: Set up a processing pipeline

Below is the `VocabExtractor.py` file containing all the necessary steps to create a Pandas DataFrame containing the entire vocabulary list.
The code should be self-explanatory, but we will highlight and explain some bits.

```{python filename="VocabExtractor.py"}
import pandas as pd
from pypdf import PdfReader


class VocabExtractor:
    def __init__(self, pdf_path):
        self.pdf_path = pdf_path

    def validate_lines(self, lines):
        return [line for line in lines if line and line[0].isdigit()]

    def remove_overflow_lines(self, lines):  # <1>
        res = [lines[0]]  # <1>
        for current_item, next_item in zip(lines, lines[1:]):  # <1>
            if next_item[0].isdigit():  # <1>
                res.append(next_item)  # <1>
            else:  # <1>
                res[-1] += next_item  # <1>
        return res  # <1>

    def trim_index(self, lines):  # <2>
        no_index_lines = [line[line.find(' '):] for line in lines]  # <2>
        return [line.strip() for line in no_index_lines]  # <2>

    def lines_to_df(self, lines):
        split_lines = [line.split("  ") for line in lines]
        words = [line[0] for line in split_lines]
        meanings = [''.join(line[1:]).strip() for line in split_lines]
        return pd.DataFrame.from_dict({"Words": words, "Meanings": meanings})

    def pipeline_lines(self, text):
        lines = text.splitlines()
        page_lines = self.validate_lines(lines)
        no_overflow_lines = self.remove_overflow_lines(page_lines)
        no_index_lines = self.trim_index(no_overflow_lines)
        return self.lines_to_df(no_index_lines)

    def extract_from_pdf(self):  # <3>
        reader = PdfReader(self.pdf_path)  # <3>
        pages = reader.pages  # <3>

        res = []  # <3>

        for page in pages:  # <3>
            page_text = page.extract_text(extraction_mode="layout")  # <3>
            res.append(self.pipeline_lines(page_text))  # <3>

        df = pd.concat(res, ignore_index=True)  # <3>
        df = df[df["Words"] != "Derde Ronde Nederlands voor buitenlanders"].reset_index(
            drop=True)  # <3>

        return df  # <3>
```

1.  Initialise a list of which its only element is the first line of extracted text from the page. Then loop over the pairs of subsequent item pairs and check if the second element of the pair starts with a digit. If it does, then there is no overflow and the succeeding element is a valid new line of text which we append to the initial list. If it does not, then it means the line was overflown and we add this newline to the final element of the initial list.
2.  For each line, extract the substring starting from the first 'word' following the first whitespace character. Effectively it removes the first word from each line, which should really be the index of the line.
3.  Combine all methods defined above and loop through the pages to create a dataframe for each page. Finally concatenate all these dataframes and filter the lines which contain the 'word' "Derde Ronde Nederlands voor buitenlanders", as it is noise from the footer that appear on every even page.[^1]

[^1]: It would also have been possible to crop the page before extracting the text, but to me it seemed like more work experimenting with the dimensions.

Now we can run the following code:

```{python}
vocab_extractor = VocabExtractor("vocab.pdf")
extracted_data = vocab_extractor.extract_from_pdf()
print(extracted_data)
```

The PDF also contained 1769 words.
Looks good to me!

## Step 5: Incorporating a LLM
